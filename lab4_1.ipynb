{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmitr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dmitr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Розмір пакету для навчання.\n",
    "epochs = 100  # Кількість епох для навчання.\n",
    "latent_dim = 256  # Розмірність прихованого простору кодування.\n",
    "num_samples = 150000  # Кількість зразків для навчання.\n",
    "# Шлях до текстового файлу даних на диску.\n",
    "data_path = os.path.join(\"ukr.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кількість зразків: 150000\n",
      "Кількість унікальних вхідних токенів: 84\n",
      "Кількість унікальних вихідних токенів: 148\n",
      "Максимальна довжина послідовності для входів: 44\n",
      "Максимальна довжина послідовності для виходів: 77\n"
     ]
    }
   ],
   "source": [
    "# Векторизуємо дані.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "# Відкриваємо файл з даними і читаємо його рядок за рядком.\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "\n",
    "# Розбиваємо кожен рядок на вхідний текст (input_text) та цільовий текст (target_text).\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "\n",
    "    # Додаємо спеціальні символи \"початку послідовності\" та \"кінця послідовності\" до цільового тексту.\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "\n",
    "    # Оновлюємо набори символів.\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "# Сортуємо символи.\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "\n",
    "# Обчислюємо різні параметри даних.\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "# Виводимо статистику.\n",
    "print(\"Кількість зразків:\", len(input_texts))\n",
    "print(\"Кількість унікальних вхідних токенів:\", num_encoder_tokens)\n",
    "print(\"Кількість унікальних вихідних токенів:\", num_decoder_tokens)\n",
    "print(\"Максимальна довжина послідовності для входів:\", max_encoder_seq_length)\n",
    "print(\"Максимальна довжина послідовності для виходів:\", max_decoder_seq_length)\n",
    "\n",
    "# Створюємо словники для перетворення символів у індекси та навпаки.\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# Ініціалізуємо дані для кодера та декодера.\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "# Заповнюємо дані для кодера та декодера.\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data випереджає decoder_input_data на один крок часу\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data буде випереджати на один крок часу\n",
    "            # і не буде включати початковий символ.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dmitr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Визначаємо вхідну послідовність та обробляємо її.\n",
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# Ми відкидаємо `encoder_outputs` і зберігаємо лише стани.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Налаштовуємо декодер, використовуючи `encoder_states` як початковий стан.\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# Ми налаштовуємо наш декодер повертати повні вихідні послідовності,\n",
    "# а також повертати внутрішні стани. Ми не використовуємо\n",
    "# повернені стани в моделі навчання, але ми будемо використовувати їх при виведенні.\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Визначаємо модель, яка перетворить\n",
    "# `encoder_input_data` та `decoder_input_data` в `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dmitr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\dmitr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dmitr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1875/1875 [==============================] - 438s 231ms/step - loss: 0.7913 - accuracy: 0.7845 - val_loss: 0.9664 - val_accuracy: 0.7167\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 426s 227ms/step - loss: 0.5731 - accuracy: 0.8330 - val_loss: 0.8211 - val_accuracy: 0.7613\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 419s 223ms/step - loss: 0.5008 - accuracy: 0.8543 - val_loss: 0.7690 - val_accuracy: 0.7758\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 413s 221ms/step - loss: 0.4585 - accuracy: 0.8663 - val_loss: 0.7048 - val_accuracy: 0.7958\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 419s 223ms/step - loss: 0.4269 - accuracy: 0.8754 - val_loss: 0.6599 - val_accuracy: 0.8083\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 422s 225ms/step - loss: 0.4020 - accuracy: 0.8829 - val_loss: 0.6304 - val_accuracy: 0.8170\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 412s 220ms/step - loss: 0.3835 - accuracy: 0.8880 - val_loss: 0.5999 - val_accuracy: 0.8259\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 415s 221ms/step - loss: 0.3691 - accuracy: 0.8920 - val_loss: 0.5853 - val_accuracy: 0.8298\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 418s 223ms/step - loss: 0.3571 - accuracy: 0.8953 - val_loss: 0.5830 - val_accuracy: 0.8301\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 413s 220ms/step - loss: 0.3472 - accuracy: 0.8980 - val_loss: 0.5635 - val_accuracy: 0.8357\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 416s 222ms/step - loss: 0.3387 - accuracy: 0.9004 - val_loss: 0.5615 - val_accuracy: 0.8367\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 416s 222ms/step - loss: 0.3312 - accuracy: 0.9025 - val_loss: 0.5474 - val_accuracy: 0.8400\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 420s 224ms/step - loss: 0.3245 - accuracy: 0.9043 - val_loss: 0.5411 - val_accuracy: 0.8421\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 419s 224ms/step - loss: 0.3191 - accuracy: 0.9058 - val_loss: 0.5307 - val_accuracy: 0.8453\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 413s 220ms/step - loss: 0.3133 - accuracy: 0.9074 - val_loss: 0.5265 - val_accuracy: 0.8466\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 410s 218ms/step - loss: 0.3084 - accuracy: 0.9087 - val_loss: 0.5181 - val_accuracy: 0.8490\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 416s 222ms/step - loss: 0.3043 - accuracy: 0.9099 - val_loss: 0.5180 - val_accuracy: 0.8486\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 416s 222ms/step - loss: 0.2996 - accuracy: 0.9112 - val_loss: 0.5125 - val_accuracy: 0.8504\n",
      "Epoch 19/100\n",
      "1875/1875 [==============================] - 415s 221ms/step - loss: 0.2957 - accuracy: 0.9123 - val_loss: 0.5072 - val_accuracy: 0.8522\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - 416s 221ms/step - loss: 0.2920 - accuracy: 0.9134 - val_loss: 0.5076 - val_accuracy: 0.8519\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - 415s 221ms/step - loss: 0.2883 - accuracy: 0.9143 - val_loss: 0.5009 - val_accuracy: 0.8543\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - 419s 223ms/step - loss: 0.2850 - accuracy: 0.9152 - val_loss: 0.4992 - val_accuracy: 0.8548\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - 416s 222ms/step - loss: 0.2816 - accuracy: 0.9162 - val_loss: 0.4971 - val_accuracy: 0.8551\n",
      "Epoch 24/100\n",
      "1875/1875 [==============================] - 414s 221ms/step - loss: 0.2785 - accuracy: 0.9171 - val_loss: 0.4940 - val_accuracy: 0.8566\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - 416s 222ms/step - loss: 0.2756 - accuracy: 0.9179 - val_loss: 0.4909 - val_accuracy: 0.8569\n",
      "Epoch 26/100\n",
      "1875/1875 [==============================] - 414s 221ms/step - loss: 0.2729 - accuracy: 0.9186 - val_loss: 0.4916 - val_accuracy: 0.8572\n",
      "Epoch 27/100\n",
      "1875/1875 [==============================] - 416s 222ms/step - loss: 0.2705 - accuracy: 0.9193 - val_loss: 0.4895 - val_accuracy: 0.8578\n",
      "Epoch 28/100\n",
      "1875/1875 [==============================] - 405s 216ms/step - loss: 0.2681 - accuracy: 0.9200 - val_loss: 0.4876 - val_accuracy: 0.8583\n",
      "Epoch 29/100\n",
      "1875/1875 [==============================] - 404s 215ms/step - loss: 0.2659 - accuracy: 0.9206 - val_loss: 0.4833 - val_accuracy: 0.8597\n",
      "Epoch 30/100\n",
      "1875/1875 [==============================] - 406s 216ms/step - loss: 0.2638 - accuracy: 0.9212 - val_loss: 0.4856 - val_accuracy: 0.8594\n",
      "Epoch 31/100\n",
      "1875/1875 [==============================] - 403s 215ms/step - loss: 0.2619 - accuracy: 0.9217 - val_loss: 0.4818 - val_accuracy: 0.8608\n",
      "Epoch 32/100\n",
      "1875/1875 [==============================] - 402s 215ms/step - loss: 0.2597 - accuracy: 0.9223 - val_loss: 0.4833 - val_accuracy: 0.8606\n",
      "Epoch 33/100\n",
      "1875/1875 [==============================] - 402s 214ms/step - loss: 0.2578 - accuracy: 0.9229 - val_loss: 0.4805 - val_accuracy: 0.8609\n",
      "Epoch 34/100\n",
      "1875/1875 [==============================] - 404s 216ms/step - loss: 0.2560 - accuracy: 0.9234 - val_loss: 0.4802 - val_accuracy: 0.8613\n",
      "Epoch 35/100\n",
      "1875/1875 [==============================] - 403s 215ms/step - loss: 0.2543 - accuracy: 0.9239 - val_loss: 0.4794 - val_accuracy: 0.8616\n",
      "Epoch 36/100\n",
      "1875/1875 [==============================] - 415s 221ms/step - loss: 0.2526 - accuracy: 0.9244 - val_loss: 0.4838 - val_accuracy: 0.8602\n",
      "Epoch 37/100\n",
      "1875/1875 [==============================] - 407s 217ms/step - loss: 0.2512 - accuracy: 0.9248 - val_loss: 0.4795 - val_accuracy: 0.8618\n",
      "Epoch 38/100\n",
      "1875/1875 [==============================] - 403s 215ms/step - loss: 0.2495 - accuracy: 0.9253 - val_loss: 0.4781 - val_accuracy: 0.8623\n",
      "Epoch 39/100\n",
      "1875/1875 [==============================] - 406s 217ms/step - loss: 0.2481 - accuracy: 0.9256 - val_loss: 0.4766 - val_accuracy: 0.8625\n",
      "Epoch 40/100\n",
      "1875/1875 [==============================] - 400s 213ms/step - loss: 0.2466 - accuracy: 0.9261 - val_loss: 0.4779 - val_accuracy: 0.8627\n",
      "Epoch 41/100\n",
      "1875/1875 [==============================] - 407s 217ms/step - loss: 0.2452 - accuracy: 0.9265 - val_loss: 0.4784 - val_accuracy: 0.8628\n",
      "Epoch 42/100\n",
      "1875/1875 [==============================] - 405s 216ms/step - loss: 0.2439 - accuracy: 0.9269 - val_loss: 0.4777 - val_accuracy: 0.8627\n",
      "Epoch 43/100\n",
      "1875/1875 [==============================] - 408s 218ms/step - loss: 0.2425 - accuracy: 0.9273 - val_loss: 0.4769 - val_accuracy: 0.8630\n",
      "Epoch 44/100\n",
      "1875/1875 [==============================] - 403s 215ms/step - loss: 0.2412 - accuracy: 0.9276 - val_loss: 0.4795 - val_accuracy: 0.8626\n",
      "Epoch 45/100\n",
      "1875/1875 [==============================] - 406s 216ms/step - loss: 0.2400 - accuracy: 0.9280 - val_loss: 0.4753 - val_accuracy: 0.8638\n",
      "Epoch 46/100\n",
      "1875/1875 [==============================] - 406s 216ms/step - loss: 0.2388 - accuracy: 0.9283 - val_loss: 0.4777 - val_accuracy: 0.8636\n",
      "Epoch 47/100\n",
      "1875/1875 [==============================] - 407s 217ms/step - loss: 0.2377 - accuracy: 0.9287 - val_loss: 0.4782 - val_accuracy: 0.8628\n",
      "Epoch 48/100\n",
      "1875/1875 [==============================] - 406s 216ms/step - loss: 0.2364 - accuracy: 0.9290 - val_loss: 0.4774 - val_accuracy: 0.8636\n",
      "Epoch 49/100\n",
      "1875/1875 [==============================] - 411s 219ms/step - loss: 0.2353 - accuracy: 0.9293 - val_loss: 0.4762 - val_accuracy: 0.8640\n",
      "Epoch 50/100\n",
      "1875/1875 [==============================] - 409s 218ms/step - loss: 0.2342 - accuracy: 0.9296 - val_loss: 0.4817 - val_accuracy: 0.8631\n",
      "Epoch 51/100\n",
      "1875/1875 [==============================] - 407s 217ms/step - loss: 0.2332 - accuracy: 0.9299 - val_loss: 0.4780 - val_accuracy: 0.8636\n",
      "Epoch 52/100\n",
      "1875/1875 [==============================] - 409s 218ms/step - loss: 0.2322 - accuracy: 0.9302 - val_loss: 0.4790 - val_accuracy: 0.8634\n",
      "Epoch 53/100\n",
      "1875/1875 [==============================] - 408s 218ms/step - loss: 0.2312 - accuracy: 0.9305 - val_loss: 0.4772 - val_accuracy: 0.8639\n",
      "Epoch 54/100\n",
      "1875/1875 [==============================] - 406s 217ms/step - loss: 0.2302 - accuracy: 0.9308 - val_loss: 0.4784 - val_accuracy: 0.8641\n",
      "Epoch 55/100\n",
      "1875/1875 [==============================] - 407s 217ms/step - loss: 0.2294 - accuracy: 0.9310 - val_loss: 0.4800 - val_accuracy: 0.8637\n",
      "Epoch 56/100\n",
      "1875/1875 [==============================] - 404s 215ms/step - loss: 0.2283 - accuracy: 0.9313 - val_loss: 0.4810 - val_accuracy: 0.8642\n",
      "Epoch 57/100\n",
      "1875/1875 [==============================] - 400s 214ms/step - loss: 0.2276 - accuracy: 0.9315 - val_loss: 0.4785 - val_accuracy: 0.8641\n",
      "Epoch 58/100\n",
      "1875/1875 [==============================] - 404s 216ms/step - loss: 0.2266 - accuracy: 0.9319 - val_loss: 0.4802 - val_accuracy: 0.8642\n",
      "Epoch 59/100\n",
      "1875/1875 [==============================] - 402s 214ms/step - loss: 0.2258 - accuracy: 0.9321 - val_loss: 0.4815 - val_accuracy: 0.8641\n",
      "Epoch 60/100\n",
      "1875/1875 [==============================] - 402s 214ms/step - loss: 0.2249 - accuracy: 0.9322 - val_loss: 0.4805 - val_accuracy: 0.8643\n",
      "Epoch 61/100\n",
      "1875/1875 [==============================] - 402s 215ms/step - loss: 0.2240 - accuracy: 0.9326 - val_loss: 0.4830 - val_accuracy: 0.8637\n",
      "Epoch 62/100\n",
      "1875/1875 [==============================] - 405s 216ms/step - loss: 0.2233 - accuracy: 0.9328 - val_loss: 0.4834 - val_accuracy: 0.8641\n",
      "Epoch 63/100\n",
      "1875/1875 [==============================] - 406s 217ms/step - loss: 0.2224 - accuracy: 0.9330 - val_loss: 0.4811 - val_accuracy: 0.8642\n",
      "Epoch 64/100\n",
      "1875/1875 [==============================] - 405s 216ms/step - loss: 0.2217 - accuracy: 0.9332 - val_loss: 0.4819 - val_accuracy: 0.8638\n",
      "Epoch 65/100\n",
      "1875/1875 [==============================] - 403s 215ms/step - loss: 0.2209 - accuracy: 0.9334 - val_loss: 0.4826 - val_accuracy: 0.8642\n",
      "Epoch 66/100\n",
      "1875/1875 [==============================] - 402s 215ms/step - loss: 0.2203 - accuracy: 0.9336 - val_loss: 0.4823 - val_accuracy: 0.8647\n",
      "Epoch 67/100\n",
      "1875/1875 [==============================] - 404s 215ms/step - loss: 0.2196 - accuracy: 0.9338 - val_loss: 0.4814 - val_accuracy: 0.8647\n",
      "Epoch 68/100\n",
      "1875/1875 [==============================] - 407s 217ms/step - loss: 0.2189 - accuracy: 0.9340 - val_loss: 0.4847 - val_accuracy: 0.8639\n",
      "Epoch 69/100\n",
      "1875/1875 [==============================] - 405s 216ms/step - loss: 0.2181 - accuracy: 0.9342 - val_loss: 0.4849 - val_accuracy: 0.8636\n",
      "Epoch 70/100\n",
      "1875/1875 [==============================] - 404s 215ms/step - loss: 0.2175 - accuracy: 0.9344 - val_loss: 0.4880 - val_accuracy: 0.8630\n",
      "Epoch 71/100\n",
      "1875/1875 [==============================] - 408s 217ms/step - loss: 0.2168 - accuracy: 0.9346 - val_loss: 0.4857 - val_accuracy: 0.8641\n",
      "Epoch 72/100\n",
      "1875/1875 [==============================] - 411s 219ms/step - loss: 0.2161 - accuracy: 0.9348 - val_loss: 0.4847 - val_accuracy: 0.8646\n",
      "Epoch 73/100\n",
      "1875/1875 [==============================] - 408s 217ms/step - loss: 0.2155 - accuracy: 0.9350 - val_loss: 0.4854 - val_accuracy: 0.8643\n",
      "Epoch 74/100\n",
      "1875/1875 [==============================] - 405s 216ms/step - loss: 0.2150 - accuracy: 0.9352 - val_loss: 0.4882 - val_accuracy: 0.8636\n",
      "Epoch 75/100\n",
      "1875/1875 [==============================] - 409s 218ms/step - loss: 0.2143 - accuracy: 0.9353 - val_loss: 0.4857 - val_accuracy: 0.8646\n",
      "Epoch 76/100\n",
      "1875/1875 [==============================] - 411s 219ms/step - loss: 0.2137 - accuracy: 0.9355 - val_loss: 0.4878 - val_accuracy: 0.8639\n",
      "Epoch 77/100\n",
      "1875/1875 [==============================] - 409s 217ms/step - loss: 0.2131 - accuracy: 0.9356 - val_loss: 0.4917 - val_accuracy: 0.8633\n",
      "Epoch 78/100\n",
      "1875/1875 [==============================] - 409s 218ms/step - loss: 0.2126 - accuracy: 0.9358 - val_loss: 0.4908 - val_accuracy: 0.8631\n",
      "Epoch 79/100\n",
      "1875/1875 [==============================] - 407s 217ms/step - loss: 0.2118 - accuracy: 0.9359 - val_loss: 0.4898 - val_accuracy: 0.8638\n",
      "Epoch 80/100\n",
      "1875/1875 [==============================] - 407s 217ms/step - loss: 0.2113 - accuracy: 0.9361 - val_loss: 0.4927 - val_accuracy: 0.8634\n",
      "Epoch 81/100\n",
      "1875/1875 [==============================] - 407s 217ms/step - loss: 0.2108 - accuracy: 0.9363 - val_loss: 0.4904 - val_accuracy: 0.8639\n",
      "Epoch 82/100\n",
      "1875/1875 [==============================] - 408s 217ms/step - loss: 0.2101 - accuracy: 0.9365 - val_loss: 0.4903 - val_accuracy: 0.8641\n",
      "Epoch 83/100\n",
      "1875/1875 [==============================] - 406s 217ms/step - loss: 0.2096 - accuracy: 0.9366 - val_loss: 0.4912 - val_accuracy: 0.8642\n",
      "Epoch 84/100\n",
      "1875/1875 [==============================] - 406s 217ms/step - loss: 0.2092 - accuracy: 0.9367 - val_loss: 0.4952 - val_accuracy: 0.8634\n",
      "Epoch 85/100\n",
      "1875/1875 [==============================] - 404s 215ms/step - loss: 0.2086 - accuracy: 0.9369 - val_loss: 0.4927 - val_accuracy: 0.8638\n",
      "Epoch 86/100\n",
      "1875/1875 [==============================] - 399s 213ms/step - loss: 0.2081 - accuracy: 0.9370 - val_loss: 0.4948 - val_accuracy: 0.8631\n",
      "Epoch 87/100\n",
      "1875/1875 [==============================] - 407s 217ms/step - loss: 0.2075 - accuracy: 0.9372 - val_loss: 0.4959 - val_accuracy: 0.8628\n",
      "Epoch 88/100\n",
      "1875/1875 [==============================] - 416s 222ms/step - loss: 0.2070 - accuracy: 0.9373 - val_loss: 0.4944 - val_accuracy: 0.8632\n",
      "Epoch 89/100\n",
      "1875/1875 [==============================] - 418s 223ms/step - loss: 0.2065 - accuracy: 0.9375 - val_loss: 0.4967 - val_accuracy: 0.8634\n",
      "Epoch 90/100\n",
      "1875/1875 [==============================] - 414s 221ms/step - loss: 0.2059 - accuracy: 0.9376 - val_loss: 0.4981 - val_accuracy: 0.8632\n",
      "Epoch 91/100\n",
      "1875/1875 [==============================] - 415s 221ms/step - loss: 0.2055 - accuracy: 0.9378 - val_loss: 0.4990 - val_accuracy: 0.8629\n",
      "Epoch 92/100\n",
      "1875/1875 [==============================] - 410s 219ms/step - loss: 0.2050 - accuracy: 0.9379 - val_loss: 0.4964 - val_accuracy: 0.8630\n",
      "Epoch 93/100\n",
      "1875/1875 [==============================] - 409s 218ms/step - loss: 0.2046 - accuracy: 0.9380 - val_loss: 0.4972 - val_accuracy: 0.8635\n",
      "Epoch 94/100\n",
      "1875/1875 [==============================] - 411s 219ms/step - loss: 0.2041 - accuracy: 0.9381 - val_loss: 0.4986 - val_accuracy: 0.8631\n",
      "Epoch 95/100\n",
      "1875/1875 [==============================] - 408s 218ms/step - loss: 0.2036 - accuracy: 0.9383 - val_loss: 0.4972 - val_accuracy: 0.8636\n",
      "Epoch 96/100\n",
      "1875/1875 [==============================] - 406s 216ms/step - loss: 0.2032 - accuracy: 0.9384 - val_loss: 0.4990 - val_accuracy: 0.8634\n",
      "Epoch 97/100\n",
      "1875/1875 [==============================] - 404s 215ms/step - loss: 0.2026 - accuracy: 0.9386 - val_loss: 0.4999 - val_accuracy: 0.8628\n",
      "Epoch 98/100\n",
      "1875/1875 [==============================] - 403s 215ms/step - loss: 0.2022 - accuracy: 0.9387 - val_loss: 0.5007 - val_accuracy: 0.8629\n",
      "Epoch 99/100\n",
      "1875/1875 [==============================] - 408s 217ms/step - loss: 0.2018 - accuracy: 0.9388 - val_loss: 0.5029 - val_accuracy: 0.8624\n",
      "Epoch 100/100\n",
      "1875/1875 [==============================] - 407s 217ms/step - loss: 0.2013 - accuracy: 0.9389 - val_loss: 0.5009 - val_accuracy: 0.8631\n"
     ]
    }
   ],
   "source": [
    "# Компілюємо модель. Використовуємо оптимізатор \"rmsprop\", функцію втрат \"categorical_crossentropy\" та метрику \"accuracy\".\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Навчаємо модель. Вхідні дані для кодера та декодера (`encoder_input_data`, `decoder_input_data`) перетворюються в цільові дані декодера (`decoder_target_data`).\n",
    "# Використовуємо розмір пакету, вказаний раніше (`batch_size`), та кількість епох (`epochs`).\n",
    "# Також відводимо 20% вхідних даних для валідації (`validation_split=0.2`).\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# Зберігаємо модель після навчання.\n",
    "model.save(\"s2s_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визначаємо моделі вибірки\n",
    "# Відновлюємо модель та створюємо кодер та декодер.\n",
    "model = keras.models.load_model(\"s2s_model.keras\")\n",
    "\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Індекс токенів для декодування послідовностей назад до\n",
    "# чогось зрозумілого.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Кодуємо вхід як вектори станів.\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Генеруємо порожню цільову послідовність довжиною 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Заповнюємо перший символ цільової послідовності символом початку.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Цикл вибірки для пакету послідовностей\n",
    "    # (для спрощення тут ми припускаємо пакет розміром 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value, verbose=0\n",
    "        )\n",
    "\n",
    "        # Вибираємо токен\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Умова виходу: або досягнуто максимальної довжини\n",
    "        # або знайдено символ зупинки.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Оновлюємо цільову послідовність (довжиною 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Оновлюємо стани\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Вхідне речення: How about playing some games\n",
      "Декодоване речення: Як щодо твоє ім'я?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Речення для перекладу\n",
    "my_sentence = \"How about playing some games\"\n",
    "\n",
    "# Кодуємо наше речення так само, як ми це робили під час підготовки даних.\n",
    "encoder_input_data = np.zeros(\n",
    "    (1, max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "for t, char in enumerate(my_sentence):\n",
    "    if char in input_token_index:\n",
    "        encoder_input_data[0, t, input_token_index[char]] = 1.0\n",
    "encoder_input_data[0, t + 1 :, input_token_index.get(\" \", 0)] = 1.0\n",
    "\n",
    "\n",
    "# Використовуємо модель для декодування речення\n",
    "decoded_sentence = decode_sequence(encoder_input_data[0:1])\n",
    "\n",
    "print(\"-\")\n",
    "print(\"Вхідне речення:\", my_sentence)\n",
    "print(\"Декодоване речення:\", decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Як бачимо, якось він текст перекладає, але для навчання було взято лише 150000 семплів (11.5 годин). Тому для якісного перекладу необхідно більше часу та більший датасет."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
